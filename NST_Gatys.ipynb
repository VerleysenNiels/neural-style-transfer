{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb06530-83b3-4f7e-b4b7-bde8b99f5d07",
   "metadata": {},
   "source": [
    "# Neural Style Transfer (NST)\n",
    "This notebook contains an implementation of the NST algorithm as proposed in \"A Neural Algorithm of Artistic Style\" which can be found here: https://arxiv.org/pdf/1508.06576\n",
    "The notebook serves as a playground to test the capabilities with different images, styles, configurations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b44721-4990-4823-b086-0bb060ecdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b5d68-2bca-4bdb-8292-c2ff577fb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73db3a-c5dc-4b70-bbe6-9291b36612d7",
   "metadata": {},
   "source": [
    "For NST the default model to be used for feature extraction is VGG-19 which has been pretrained for image recognition. Since it is kind of useless to train VGG again from scratch, we can use the pretrained version from the TorchVision model library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3c164-7ed2-4a75-8a46-f6693dda7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Freeze the model parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68af73-26f3-48c2-a469-512ebc69a265",
   "metadata": {},
   "source": [
    "The most important thing is of course to calculate the loss of the generated image, in order to push it towards the desired result.\n",
    "\n",
    "There are two losses that need to be combined. First, there is the content loss which directly compares the feature maps from the content and generated images. This one ensures that the output image resembles the content image in form.\n",
    "The second loss is the style loss, which compares the Gram matrices (representing the style in feature maps through repeating patterns and textures) of the style and generated images. This loss function pushes the generated image to apply the style from the style image.\n",
    "By then combining these two losses we generate an image that contains the same content as the content image, in the style of the style image. By playing with the weights of these two losses, we can control how the generated images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e090bd-6384-4d02-8a18-753a82cf58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gram_matrix(tensor):\n",
    "    # Extract dimensions\n",
    "    _, n_channels, height, width = tensor.size()\n",
    "    \n",
    "    # Flatten the feature map by reshaping it into (n_channels, height * width)\n",
    "    features = tensor.view(n_channels, height * width)\n",
    "    \n",
    "    # Calculate the Gram matrix as the product of the features with its transpose\n",
    "    gram = torch.mm(features, features.t())\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7baf66c-bcea-475d-b8c3-410cd398766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(generated_feature_maps, content_feature_maps, style_grams, alpha, beta):\n",
    "    # Calculate content loss as the MSE between the feature maps of the given layer\n",
    "    # Looking at other implementations conv4_2 seems to be sufficient for the content loss\n",
    "    content_loss = torch.nn.MSELoss(reduction='mean')(content_feature_maps['conv4_2'].squeeze(axis=0), generated_feature_maps['conv4_2'].squeeze(axis=0))\n",
    "\n",
    "    # Calculate the style loss using the Gram matrices\n",
    "    style_loss = 0\n",
    "    for layer in style_grams.keys:\n",
    "        generated_gram = calculate_gram_matrix(generated_features[layer])\n",
    "        layer_style_loss = torch.nn.MSELoss(generated_gram, style_grams[layer])\n",
    "        style_loss += layer_style_loss\n",
    "    \n",
    "    # Average style loss across layers\n",
    "    style_loss /= len(style_grams.keys)\n",
    "\n",
    "    # Combine the content and style losses with their respective weights\n",
    "    total_loss = alpha * content_loss + beta * style_loss\n",
    "\n",
    "    return total_loss, content_loss, style_loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
